{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ab95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AGENTE Q&A CONDOM√çNIO SOLAR COM LANGGRAPH E MEM√ìRIA ---\n",
    "# COM TRUNCAMENTO AUTOM√ÅTICO DE CONTEXTO\n",
    "# Autor: William Lapa\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import json\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import io\n",
    "import tiktoken\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# OCR imports\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Display imports\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- CONFIGURA√á√ïES GLOBAIS ---\n",
    "CACHE_DIR = \"processed_docs_cache\"\n",
    "DOCS_DIR = \"docs_condominio\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QASession:\n",
    "    \"\"\"Classe para gerenciar sess√£o de Q&A com mem√≥ria\"\"\"\n",
    "    conversation_history: List[Dict[str, str]]\n",
    "    retriever: Optional[Any]\n",
    "    retriever_initialized: bool\n",
    "    session_id: str\n",
    "    documents_loaded: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.conversation_history:\n",
    "            self.conversation_history = []\n",
    "        if not self.session_id:\n",
    "            self.session_id = f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# --- ESTADO DO GRAFO ---\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"Estado do agente Q&A para condom√≠nios\"\"\"\n",
    "    question: str\n",
    "    documents: Annotated[List[Document], operator.add]\n",
    "    answer: str\n",
    "    retriever: object\n",
    "    retriever_initialized: bool\n",
    "    conversation_history: List[Dict[str, str]]\n",
    "    session_context: Optional[QASession]\n",
    "\n",
    "class SolarCondominiumQA:\n",
    "    \"\"\"Agente Q&A especializado para condom√≠nio Solar com OCR e cache\"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, docs_directory: str = DOCS_DIR, provider: str = \"openai\"):\n",
    "        self.docs_directory = docs_directory\n",
    "        self.cache_dir = CACHE_DIR\n",
    "        self.provider = provider\n",
    "        self.session = QASession(\n",
    "            conversation_history=[],\n",
    "            retriever=None,\n",
    "            retriever_initialized=False,\n",
    "            session_id=\"\"        \n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Inicializar componentes\n",
    "        self._setup_directories()\n",
    "        self._initialize_llm_dynamic(self.provider)  # Escolha o LLM desejado\n",
    "        self._build_graph()\n",
    "    \n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Cria diret√≥rios necess√°rios\"\"\"\n",
    "        for directory in [self.docs_directory, self.cache_dir]:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "                print(f\"üìÅ Diret√≥rio '{directory}/' criado.\")\n",
    "    \n",
    "    def _initialize_llm_openai(self):\n",
    "        \"\"\"Inicializa LLM e embeddings\"\"\"\n",
    "        # Usando gpt-4o-mini do OpenAI\n",
    "        print(\"üîß Inicializando LLM OpenAI (gpt-4o-mini)...\")\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def _initialize_llm_gemini(self):\n",
    "        \"\"\"Vers√£o Gemini\"\"\"\n",
    "        print(\"üîß Inicializando LLM Google Gemini...\")\n",
    "        \n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-exp\",\n",
    "            temperature=0,\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "        )\n",
    "                \n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "    def _initialize_llm_claude(self):\n",
    "        \"\"\"Vers√£o Claude\"\"\"\n",
    "        print(\"üîß Inicializando LLM Anthropic Claude...\")\n",
    "        self.llm = ChatAnthropic(\n",
    "            # model=\"claude-3-5-sonnet-20241022\",\n",
    "            model=\"claude-3-7-sonnet-latest\",\n",
    "            temperature=0,\n",
    "            anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "            max_tokens=1024,  # Aumentar limite de tokens\n",
    "        )\n",
    "        # Claude n√£o tem embeddings, usar OpenAI como fallback\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def _initialize_llm_groq_llama3(self):\n",
    "        \"\"\"Vers√£o Groq (Ultra-r√°pido)\"\"\"        \n",
    "        print(\"üîß Inicializando LLM Groq (Llama 3.3)...\")\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def _initialize_llm_groq_gemma(self):\n",
    "        \"\"\"Vers√£o Groq (Ultra-r√°pido)\"\"\"        \n",
    "        print(\"üîß Inicializando LLM Groq (Gemma 2.0)...\")\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"gemma-7b-it\",\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def _initialize_llm_groq_mistral(self):\n",
    "        \"\"\"Vers√£o Groq (Ultra-r√°pido)\"\"\"        \n",
    "        print(\"üîß Inicializando LLM Groq (Mixtral 8x7B)...\" )\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "    def _initialize_llm_deepseek(self):\n",
    "        \"\"\"Vers√£o DeepSeek (OpenAI API compat√≠vel)\"\"\"\n",
    "        print(\"üîß Inicializando LLM DeepSeek (DeepSeek Chat)...\")\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"deepseek-chat\",\n",
    "            temperature=0,\n",
    "            openai_api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "            openai_api_base=\"https://api.deepseek.com\"\n",
    "        )\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # Exemplo de uso com sele√ß√£o din√¢mica\n",
    "    def _initialize_llm_dynamic(self, provider: str):\n",
    "        \"\"\"Fun√ß√£o din√¢mica que escolhe o provider\"\"\"\n",
    "        providers = {\n",
    "            \"openai\": self._initialize_llm_openai,\n",
    "            \"gemini\": self._initialize_llm_gemini,\n",
    "            \"claude\": self._initialize_llm_claude,\n",
    "            \"groq_llama3\": self._initialize_llm_groq_llama3,\n",
    "            \"groq_gemma\": self._initialize_llm_groq_gemma,\n",
    "            \"groq_mistral\": self._initialize_llm_groq_mistral,\n",
    "            \"deepseek\": self._initialize_llm_deepseek\n",
    "        }\n",
    "\n",
    "        print(f\"üîß Inicializando LLM com : {provider}\\n\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if provider in providers:\n",
    "            providers[provider]()\n",
    "        else:\n",
    "            available = \", \".join(providers.keys())\n",
    "            raise ValueError(f\"Provider '{provider}' n√£o suportado. Dispon√≠veis: {available}\")\n",
    "\n",
    "    \n",
    "    def _ocr_pdf_page(self, pdf_doc, page_number):\n",
    "        \"\"\"Realiza OCR em uma p√°gina do PDF\"\"\"\n",
    "        try:\n",
    "            page = pdf_doc.load_page(page_number)\n",
    "            pix = page.get_pixmap()\n",
    "            img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "            text = pytesseract.image_to_string(img, lang='por')\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå [OCR] Erro na p√°gina {page_number+1}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _load_documents_with_cache(self):\n",
    "        \"\"\"Carrega documentos com sistema de cache inteligente\"\"\"\n",
    "        documentos_carregados = []\n",
    "        print(f\"üìö Carregando documentos de: {self.docs_directory}\")\n",
    "        \n",
    "        for root, dirs, files in os.walk(self.docs_directory):\n",
    "            for file_name in files:\n",
    "                if file_name.lower().endswith(\".pdf\"):\n",
    "                    caminho_arquivo_pdf = os.path.join(root, file_name)\n",
    "                    cache_file_name = os.path.splitext(file_name)[0] + \".txt\"\n",
    "                    cache_file_path = os.path.join(self.cache_dir, cache_file_name)\n",
    "                    \n",
    "                    texto_completo = \"\"\n",
    "                    \n",
    "                    # Tentar carregar do cache\n",
    "                    if os.path.exists(cache_file_path):\n",
    "                        try:\n",
    "                            with open(cache_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                                texto_completo = f.read()\n",
    "                            if texto_completo.strip():\n",
    "                                documentos_carregados.append(\n",
    "                                    Document(\n",
    "                                        page_content=texto_completo, \n",
    "                                        metadata={\"source\": caminho_arquivo_pdf, \"cached\": True}\n",
    "                                    )\n",
    "                                )\n",
    "                                # print(f\"  ‚ôªÔ∏è  Cache: '{file_name}'\")\n",
    "                                continue\n",
    "                        except Exception as e:\n",
    "                            print(f\"  ‚ö†Ô∏è  Erro no cache para '{file_name}': {e}\")\n",
    "                    \n",
    "                    # Processar PDF se n√£o estiver no cache\n",
    "                    try:\n",
    "                        doc = fitz.open(caminho_arquivo_pdf)\n",
    "                        print(f\"  üìÑ Processando: '{file_name}'\")\n",
    "                        \n",
    "                        for page_num, page in enumerate(doc):\n",
    "                            page_text = page.get_text()\n",
    "                            \n",
    "                            # Se extra√ß√£o normal falhar, usar OCR\n",
    "                            if not page_text.strip():\n",
    "                                print(f\"    üîç OCR p√°gina {page_num + 1}\")\n",
    "                                ocr_text = self._ocr_pdf_page(doc, page_num)\n",
    "                                if ocr_text.strip():\n",
    "                                    page_text = ocr_text\n",
    "                            \n",
    "                            if page_text.strip():\n",
    "                                texto_completo += page_text + \"\\n\\n\"\n",
    "                        \n",
    "                        if texto_completo.strip():\n",
    "                            documentos_carregados.append(\n",
    "                                Document(\n",
    "                                    page_content=texto_completo, \n",
    "                                    metadata={\"source\": caminho_arquivo_pdf, \"cached\": False}\n",
    "                                )\n",
    "                            )\n",
    "                            \n",
    "                            # Salvar no cache\n",
    "                            with open(cache_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                                f.write(texto_completo)\n",
    "                            print(f\"    üíæ Salvo no cache\")\n",
    "                        \n",
    "                        doc.close()\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå Erro ao processar '{file_name}': {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ Total: {len(documentos_carregados)} documentos carregados\")\n",
    "        return documentos_carregados\n",
    "    \n",
    "    # --- N√ìNS DO GRAFO ---\n",
    "    def _load_documents_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"N√≥ para carregar documentos\"\"\"\n",
    "        if not os.path.exists(self.docs_directory) or not os.listdir(self.docs_directory):\n",
    "            return {\n",
    "                \"retriever_initialized\": False, \n",
    "                \"answer\": f\"‚ùå Diret√≥rio '{self.docs_directory}' vazio ou inexistente.\"\n",
    "            }\n",
    "        \n",
    "        docs = self._load_documents_with_cache()\n",
    "        \n",
    "        if not docs:\n",
    "            return {\n",
    "                \"retriever_initialized\": False, \n",
    "                \"answer\": \"‚ùå Nenhum documento v√°lido encontrado.\"\n",
    "            }\n",
    "        \n",
    "        # Dividir documentos\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, \n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        \n",
    "        # Criar retriever\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=split_docs, \n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "        \n",
    "        # Atualizar sess√£o\n",
    "        self.session.retriever = retriever\n",
    "        self.session.retriever_initialized = True\n",
    "        self.session.documents_loaded = True\n",
    "        \n",
    "        return {\n",
    "            \"documents\": split_docs,\n",
    "            \"retriever\": retriever,\n",
    "            \"retriever_initialized\": True,\n",
    "            \"answer\": \"\"\n",
    "        }\n",
    "    \n",
    "    def _retrieve_documents_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"N√≥ para buscar documentos relevantes\"\"\"\n",
    "        question = state[\"question\"]\n",
    "        \n",
    "        # Usar retriever da sess√£o se dispon√≠vel\n",
    "        retriever = self.session.retriever if self.session.retriever_initialized else state.get(\"retriever\")\n",
    "        \n",
    "        if retriever is None:\n",
    "            return {\n",
    "                \"documents\": [], \n",
    "                \"answer\": \"‚ùå Retriever n√£o configurado.\"\n",
    "            }\n",
    "        \n",
    "        documents_for_qa = retriever.invoke(question)\n",
    "        return {\"documents\": documents_for_qa}\n",
    "    \n",
    "    def _filter_relevant_documents(self, documents: List[Document], question: str) -> List[Document]:\n",
    "        \"\"\"Filtra documentos por relev√¢ncia usando embeddings\"\"\"\n",
    "        question_embedding = self.embeddings.embed_query(question)\n",
    "        scored_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_embedding = self.embeddings.embed_query(doc.page_content)\n",
    "            similarity = np.dot(question_embedding, doc_embedding)\n",
    "            scored_docs.append((similarity, doc))\n",
    "        \n",
    "        # Ordena por similaridade e pega os mais relevantes\n",
    "        scored_docs.sort(reverse=True, key=lambda x: x[0])\n",
    "        return [doc for _, doc in scored_docs[:3]]  # Apenas os 3 mais relevantes\n",
    "    \n",
    "    def _generate_answer_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"N√≥ para gerar resposta\"\"\"\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "        \n",
    "        if not documents:\n",
    "            return {\"answer\": \"‚ùå Nenhum documento relevante encontrado.\"}\n",
    "        \n",
    "        # Prompt especializado para condom√≠nio\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Voc√™ √© um assistente especializado em Q&A para o Condom√≠nio Solar Trindade.\n",
    "        Use os documentos fornecidos (atas, contratos, comunicados, regulamentos) para responder √† pergunta.\n",
    "        \n",
    "        INSTRU√á√ïES:\n",
    "        - Seja preciso e espec√≠fico nas datas e informa√ß√µes\n",
    "        - Se n√£o souber, diga \"N√£o tenho informa√ß√µes suficientes nos documentos\"\n",
    "        - Para listas, use formata√ß√£o clara com numera√ß√£o\n",
    "        - Mantenha tom profissional mas acess√≠vel\n",
    "        \n",
    "        Contexto dos documentos: {context}\n",
    "        \n",
    "        Pergunta: {input}\n",
    "        \n",
    "        Resposta:\n",
    "        \"\"\")\n",
    "        \n",
    "        document_chain = create_stuff_documents_chain(self.llm, prompt)\n",
    "        response = document_chain.invoke({\"input\": question, \"context\": documents}) ## SEM TRUNCAMENTO\n",
    "        \n",
    "        return {\"answer\": response}\n",
    "    \n",
    "    def _decide_next_step(self, state: AgentState) -> str:\n",
    "        \"\"\"Decide pr√≥ximo passo baseado no estado\"\"\"\n",
    "        # Verificar se retriever est√° inicializado na sess√£o ou no estado\n",
    "        if self.session.retriever_initialized or state.get(\"retriever_initialized\"):\n",
    "            return \"retrieve_docs\"\n",
    "        else:\n",
    "            return \"load_docs\"\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Constr√≥i o grafo LangGraph\"\"\"\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Adicionar n√≥s\n",
    "        workflow.add_node(\"decide_initial_step\", lambda state: state)\n",
    "        workflow.add_node(\"load_docs\", self._load_documents_node)\n",
    "        workflow.add_node(\"retrieve_docs\", self._retrieve_documents_node)\n",
    "        workflow.add_node(\"generate_answer\", self._generate_answer_node)\n",
    "        \n",
    "        # Configurar fluxo\n",
    "        workflow.set_entry_point(\"decide_initial_step\")\n",
    "        \n",
    "        workflow.add_conditional_edges(\n",
    "            \"decide_initial_step\",\n",
    "            self._decide_next_step,\n",
    "            {\n",
    "                \"load_docs\": \"load_docs\",\n",
    "                \"retrieve_docs\": \"retrieve_docs\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        workflow.add_edge(\"load_docs\", \"retrieve_docs\")\n",
    "        workflow.add_edge(\"retrieve_docs\", \"generate_answer\")\n",
    "        workflow.add_edge(\"generate_answer\", END)\n",
    "        \n",
    "        self.app = workflow.compile()\n",
    "    \n",
    "    # --- INTERFACE P√öBLICA ---\n",
    "    def ask_question(self, question: str, show_process: bool = False) -> str:\n",
    "        \"\"\"Faz uma pergunta ao agente\"\"\"\n",
    "        print(f\"‚ùì {question}\")\n",
    "        \n",
    "        if show_process:\n",
    "            print(\"üîÑ Processando...\")\n",
    "        \n",
    "        # Preparar estado inicial\n",
    "        initial_state = {\n",
    "            \"question\": question,\n",
    "            \"documents\": [],\n",
    "            \"answer\": \"\",\n",
    "            \"retriever\": self.session.retriever,\n",
    "            \"retriever_initialized\": self.session.retriever_initialized,\n",
    "            \"conversation_history\": self.session.conversation_history,\n",
    "            \"session_context\": self.session\n",
    "        }\n",
    "        \n",
    "        # Executar grafo\n",
    "        current_result = None\n",
    "        for step in self.app.stream(initial_state):\n",
    "            if show_process:\n",
    "                node_name = list(step.keys())[0] if step else \"unknown\"\n",
    "                print(f\"  üî∏ {node_name}\")\n",
    "            current_result = step\n",
    "        \n",
    "        # Processar resultado\n",
    "        if current_result:\n",
    "            final_state = list(current_result.values())[0]\n",
    "            answer = final_state.get(\"answer\", \"‚ùå Erro ao processar pergunta\")\n",
    "            \n",
    "            # Atualizar hist√≥rico\n",
    "            qa_pair = {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            self.session.conversation_history.append(qa_pair)\n",
    "            \n",
    "            return answer\n",
    "        \n",
    "        return \"‚ùå Erro no processamento\"\n",
    "    \n",
    "    def ask_and_display(self, question: str, show_process: bool = False):\n",
    "        \"\"\"Faz pergunta e exibe resposta formatada\"\"\"\n",
    "        answer = self.ask_question(question, show_process)\n",
    "        print(f\"\\nüí° **Resposta:**\")\n",
    "        display(Markdown(answer))\n",
    "        return answer\n",
    "    \n",
    "    def show_conversation_history(self, limit: int = 5):\n",
    "        \"\"\"Mostra hist√≥rico de conversas\"\"\"\n",
    "        print(f\"\\nüìã **Hist√≥rico de Conversas** (√∫ltimas {limit}):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        recent = self.session.conversation_history[-limit:]\n",
    "        \n",
    "        for i, qa in enumerate(recent, 1):\n",
    "            timestamp = qa.get(\"timestamp\", \"N/A\")\n",
    "            question = qa.get(\"question\", \"\")\n",
    "            answer = qa.get(\"answer\", \"\")\n",
    "            \n",
    "            print(f\"\\n**{i}.** üïí {timestamp}\")\n",
    "            print(f\"‚ùì {question}\")\n",
    "            print(f\"üí° {answer[:100]}{'...' if len(answer) > 100 else ''}\")\n",
    "    \n",
    "    def get_session_info(self):\n",
    "        \"\"\"Informa√ß√µes da sess√£o\"\"\"\n",
    "        return {\n",
    "            \"session_id\": self.session.session_id,\n",
    "            \"total_questions\": len(self.session.conversation_history),\n",
    "            \"retriever_initialized\": self.session.retriever_initialized,\n",
    "            \"documents_loaded\": self.session.documents_loaded,\n",
    "            \"docs_directory\": self.docs_directory,\n",
    "            \"cache_directory\": self.cache_dir\n",
    "        }\n",
    "\n",
    "# --- EXECU√á√ÉO PRINCIPAL ---\n",
    "def run_solar_qa_demo(provider: str = \"openai\"):\n",
    "    \"\"\"Executa demonstra√ß√£o do agente Solar Q&A\"\"\"\n",
    "    \n",
    "    print(\"üè¢ === AGENTE Q&A CONDOM√çNIO SOLAR TRINDADE ===\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Inicializar agente\n",
    "    qa_agent = SolarCondominiumQA(provider=provider)\n",
    "    \n",
    "    # Perguntas de exemplo\n",
    "    sample_questions = [\n",
    "        \"Qual a data da √∫ltima reuni√£o ou assembleia do condom√≠nio?\",\n",
    "        #\"Quais as datas das cinco √∫ltimas reuni√µes ou assembleias?\",\n",
    "        #\"Quais foram os principais assuntos da √∫ltima assembleia?\",\n",
    "        #\"quais os nomes dos membros do conselho fiscal atuais?\",\n",
    "        #\"Como base na ultima assembleia, qual o valor da taxa condominial atual?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìä **Informa√ß√µes da Sess√£o:**\")\n",
    "    for key, value in qa_agent.get_session_info().items():\n",
    "        print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Executar perguntas\n",
    "    for i, question in enumerate(sample_questions, 1):\n",
    "        print(f\"\\nüî∏ **Pergunta {i}/{len(sample_questions)}**\")\n",
    "        qa_agent.ask_and_display(question, show_process=(i == 1))\n",
    "        \n",
    "        if i < len(sample_questions):\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "    \n",
    "    # Mostrar hist√≥rico\n",
    "    qa_agent.show_conversation_history()\n",
    "    \n",
    "    print(f\"\\n‚úÖ **Sess√£o Finalizada:** {qa_agent.session.session_id}\")\n",
    "    print(f\"üìà **Total de Perguntas:** {len(qa_agent.session.conversation_history)}\")\n",
    "    \n",
    "    return qa_agent\n",
    "\n",
    "def interactive_solar_qa():\n",
    "    \"\"\"Modo interativo para o Solar Q&A\"\"\"\n",
    "    qa_agent = SolarCondominiumQA()\n",
    "    \n",
    "    print(\"üè¢ === MODO INTERATIVO - SOLAR TRINDADE ===\")\n",
    "    print(\"Comandos: 'sair', 'historico', 'info', 'limpar'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\n‚ùì Sua pergunta: \").strip()\n",
    "            \n",
    "            if question.lower() in ['sair', 'exit', 'quit']:\n",
    "                print(\"üëã Encerrando...\")\n",
    "                break\n",
    "            elif question.lower() in ['historico', 'history']:\n",
    "                qa_agent.show_conversation_history()\n",
    "                continue\n",
    "            elif question.lower() == 'info':\n",
    "                print(\"\\nüìä **Informa√ß√µes da Sess√£o:**\")\n",
    "                for key, value in qa_agent.get_session_info().items():\n",
    "                    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "                continue\n",
    "            elif question.lower() in ['limpar', 'clear']:\n",
    "                qa_agent.session.conversation_history = []\n",
    "                print(\"üóëÔ∏è Hist√≥rico limpo\")\n",
    "                continue\n",
    "            elif not question:\n",
    "                continue\n",
    "            \n",
    "            qa_agent.ask_and_display(question)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Interrompido pelo usu√°rio\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro: {e}\")\n",
    "    \n",
    "    return qa_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- EXECU√á√ÉO ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Escolha o modo de execu√ß√£o:\n",
    "    # qa_agent = run_solar_qa_demo(\"claude\")     # Demo autom√°tica\n",
    "    # qa_agent = interactive_solar_qa()   # Modo interativo\n",
    "\n",
    "    providers = [\n",
    "        # \"openai\", \"gemini\", \n",
    "        # \"claude\",\n",
    "        # \"groq_llama3\", \n",
    "        # \"groq_gemma\", \n",
    "        # \"groq_mistral\", \n",
    "        \"deepseek\"\n",
    "    ]\n",
    "\n",
    "    for provider in providers:\n",
    "        print(f\"\\nüîß Inicializando LLM: {provider}\")\n",
    "        run_solar_qa_demo(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_agent = SolarCondominiumQA()\n",
    "# resposta = qa_agent.ask_question(\"Qual a √∫ltima assembleia? Resposta curta e resumida\")\n",
    "resposta = qa_agent.ask_question(\"Quem s√£o os membros do conselho fiscal? Resposta curta e resumida\")\n",
    "qa_agent.show_conversation_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
