{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150ab95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AGENTE Q&A CONDOMÃNIO SOLAR COM LANGGRAPH E MEMÃ“RIA ---\n",
    "# COM TRUNCAMENTO AUTOMÃTICO DE CONTEXTO\n",
    "# Autor: William Lapa\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import json\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import io\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# OCR imports\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Display imports\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- CONFIGURAÃ‡Ã•ES GLOBAIS ---\n",
    "CACHE_DIR = \"processed_docs_cache\"\n",
    "DOCS_DIR = \"docs_condominio\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QASession:\n",
    "    \"\"\"Classe para gerenciar sessÃ£o de Q&A com memÃ³ria\"\"\"\n",
    "    conversation_history: List[Dict[str, str]]\n",
    "    retriever: Optional[Any]\n",
    "    retriever_initialized: bool\n",
    "    session_id: str\n",
    "    documents_loaded: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.conversation_history:\n",
    "            self.conversation_history = []\n",
    "        if not self.session_id:\n",
    "            self.session_id = f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# --- ESTADO DO GRAFO ---\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"Estado do agente Q&A para condomÃ­nios\"\"\"\n",
    "    question: str\n",
    "    documents: Annotated[List[Document], operator.add]\n",
    "    answer: str\n",
    "    retriever: object\n",
    "    retriever_initialized: bool\n",
    "    conversation_history: List[Dict[str, str]]\n",
    "    session_context: Optional[QASession]\n",
    "\n",
    "\n",
    "class SolarCondominiumQA:\n",
    "    \"\"\"Agente Q&A especializado para condomÃ­nio Solar com OCR e cache\"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, docs_directory: str = DOCS_DIR, provider: str = \"openai\"):\n",
    "        self.docs_directory = docs_directory\n",
    "        self.cache_dir = CACHE_DIR\n",
    "        self.provider = provider\n",
    "        self.session = QASession(\n",
    "            conversation_history=[],\n",
    "            retriever=None,\n",
    "            retriever_initialized=False,\n",
    "            session_id=\"\"        \n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Inicializar componentes\n",
    "        self._setup_directories()\n",
    "        self._initialize_llm_dynamic(self.provider)  # Escolha o LLM desejado\n",
    "        self._build_graph()\n",
    "\n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Conta o nÃºmero de tokens usando o codificador do modelo\"\"\"\n",
    "        try:\n",
    "            encoding = tiktoken.encoding_for_model(\"deepseek-chat\")\n",
    "            return len(encoding.encode(text))\n",
    "        except:\n",
    "            # Fallback simples se nÃ£o conseguir contar tokens\n",
    "            return len(text.split()) // 3  # AproximaÃ§Ã£o grosseira\n",
    "\n",
    "    def _truncate_documents(self, documents: List[Document], max_tokens: int = 60000) -> List[Document]:\n",
    "        \"\"\"Trunca documentos para ficar dentro do limite de tokens\"\"\"\n",
    "        total_tokens = 0\n",
    "        truncated_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_tokens = self._count_tokens(doc.page_content)\n",
    "            if total_tokens + doc_tokens <= max_tokens:\n",
    "                truncated_docs.append(doc)\n",
    "                total_tokens += doc_tokens\n",
    "            else:\n",
    "                remaining_tokens = max_tokens - total_tokens\n",
    "                if remaining_tokens > 1000:  # SÃ³ adiciona se valer a pena\n",
    "                    # Trunca o conteÃºdo do documento\n",
    "                    truncated_content = \" \".join(doc.page_content.split()[:remaining_tokens*3])\n",
    "                    truncated_doc = Document(\n",
    "                        page_content=truncated_content,\n",
    "                        metadata=doc.metadata\n",
    "                    )\n",
    "                    truncated_docs.append(truncated_doc)\n",
    "                    break  # JÃ¡ atingiu o limite\n",
    "                    \n",
    "        print(f\"â„¹ï¸ Documentos truncados para {total_tokens} tokens (limite: {max_tokens})\")\n",
    "        return truncated_docs\n",
    "    \n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Cria diretÃ³rios necessÃ¡rios\"\"\"\n",
    "        for directory in [self.docs_directory, self.cache_dir]:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "                print(f\"ğŸ“ DiretÃ³rio '{directory}/' criado.\")\n",
    "    \n",
    "    def _initialize_llm_openai(self):\n",
    "        \"\"\"Inicializa LLM e embeddings\"\"\"\n",
    "        # Usando gpt-4o-mini do OpenAI\n",
    "        print(\"ğŸ”§ Inicializando LLM OpenAI (gpt-4o-mini)...\")\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def _initialize_llm_gemini(self):\n",
    "        \"\"\"VersÃ£o Gemini\"\"\"\n",
    "        print(\"ğŸ”§ Inicializando LLM Google Gemini...\")\n",
    "        \n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash-exp\",\n",
    "            temperature=0,\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "        )\n",
    "                \n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "    def _initialize_llm_claude(self):\n",
    "        \"\"\"VersÃ£o Claude\"\"\"\n",
    "        print(\"ğŸ”§ Inicializando LLM Anthropic Claude...\")\n",
    "        self.llm = ChatAnthropic(\n",
    "            # model=\"claude-3-5-sonnet-20241022\",\n",
    "            model=\"claude-3-7-sonnet-latest\",\n",
    "            temperature=0,\n",
    "            anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "            max_tokens=1024,  # Aumentar limite de tokens\n",
    "        )\n",
    "        # Claude nÃ£o tem embeddings, usar OpenAI como fallback\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def _initialize_llm_groq_llama3(self):\n",
    "        \"\"\"VersÃ£o Groq (Ultra-rÃ¡pido)\"\"\"        \n",
    "        print(\"ğŸ”§ Inicializando LLM Groq (Llama 3.3)...\")\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def _initialize_llm_groq_gemma(self):\n",
    "        \"\"\"VersÃ£o Groq (Ultra-rÃ¡pido)\"\"\"        \n",
    "        print(\"ğŸ”§ Inicializando LLM Groq (Gemma 2.0)...\")\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"gemma-7b-it\",\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def _initialize_llm_groq_mistral(self):\n",
    "        \"\"\"VersÃ£o Groq (Ultra-rÃ¡pido)\"\"\"        \n",
    "        print(\"ğŸ”§ Inicializando LLM Groq (Mixtral 8x7B)...\" )\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "    def _initialize_llm_deepseek(self):\n",
    "        \"\"\"VersÃ£o DeepSeek (OpenAI API compatÃ­vel)\"\"\"\n",
    "        print(\"ğŸ”§ Inicializando LLM DeepSeek (DeepSeek Chat)...\")\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"deepseek-chat\",\n",
    "            temperature=0,\n",
    "            openai_api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "            openai_api_base=\"https://api.deepseek.com\"\n",
    "        )\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # Exemplo de uso com seleÃ§Ã£o dinÃ¢mica\n",
    "    def _initialize_llm_dynamic(self, provider: str):\n",
    "        \"\"\"FunÃ§Ã£o dinÃ¢mica que escolhe o provider\"\"\"\n",
    "        providers = {\n",
    "            \"openai\": self._initialize_llm_openai,\n",
    "            \"gemini\": self._initialize_llm_gemini,\n",
    "            \"claude\": self._initialize_llm_claude,\n",
    "            \"groq_llama3\": self._initialize_llm_groq_llama3,\n",
    "            \"groq_gemma\": self._initialize_llm_groq_gemma,\n",
    "            \"groq_mistral\": self._initialize_llm_groq_mistral,\n",
    "            \"deepseek\": self._initialize_llm_deepseek\n",
    "        }\n",
    "\n",
    "        print(f\"ğŸ”§ Inicializando LLM com : {provider}\\n\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if provider in providers:\n",
    "            providers[provider]()\n",
    "        else:\n",
    "            available = \", \".join(providers.keys())\n",
    "            raise ValueError(f\"Provider '{provider}' nÃ£o suportado. DisponÃ­veis: {available}\")\n",
    "\n",
    "    \n",
    "    def _ocr_pdf_page(self, pdf_doc, page_number):\n",
    "        \"\"\"Realiza OCR em uma pÃ¡gina do PDF\"\"\"\n",
    "        try:\n",
    "            page = pdf_doc.load_page(page_number)\n",
    "            pix = page.get_pixmap()\n",
    "            img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "            text = pytesseract.image_to_string(img, lang='por')\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ [OCR] Erro na pÃ¡gina {page_number+1}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _load_documents_with_cache(self):\n",
    "        \"\"\"Carrega documentos com sistema de cache inteligente\"\"\"\n",
    "        documentos_carregados = []\n",
    "        print(f\"ğŸ“š Carregando documentos de: {self.docs_directory}\")\n",
    "        \n",
    "        for root, dirs, files in os.walk(self.docs_directory):\n",
    "            for file_name in files:\n",
    "                if file_name.lower().endswith(\".pdf\"):\n",
    "                    caminho_arquivo_pdf = os.path.join(root, file_name)\n",
    "                    cache_file_name = os.path.splitext(file_name)[0] + \".txt\"\n",
    "                    cache_file_path = os.path.join(self.cache_dir, cache_file_name)\n",
    "                    \n",
    "                    texto_completo = \"\"\n",
    "                    \n",
    "                    # Tentar carregar do cache\n",
    "                    if os.path.exists(cache_file_path):\n",
    "                        try:\n",
    "                            with open(cache_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                                texto_completo = f.read()\n",
    "                            if texto_completo.strip():\n",
    "                                documentos_carregados.append(\n",
    "                                    Document(\n",
    "                                        page_content=texto_completo, \n",
    "                                        metadata={\"source\": caminho_arquivo_pdf, \"cached\": True}\n",
    "                                    )\n",
    "                                )\n",
    "                                # print(f\"  â™»ï¸  Cache: '{file_name}'\")\n",
    "                                continue\n",
    "                        except Exception as e:\n",
    "                            print(f\"  âš ï¸  Erro no cache para '{file_name}': {e}\")\n",
    "                    \n",
    "                    # Processar PDF se nÃ£o estiver no cache\n",
    "                    try:\n",
    "                        doc = fitz.open(caminho_arquivo_pdf)\n",
    "                        print(f\"  ğŸ“„ Processando: '{file_name}'\")\n",
    "                        \n",
    "                        for page_num, page in enumerate(doc):\n",
    "                            page_text = page.get_text()\n",
    "                            \n",
    "                            # Se extraÃ§Ã£o normal falhar, usar OCR\n",
    "                            if not page_text.strip():\n",
    "                                print(f\"    ğŸ” OCR pÃ¡gina {page_num + 1}\")\n",
    "                                ocr_text = self._ocr_pdf_page(doc, page_num)\n",
    "                                if ocr_text.strip():\n",
    "                                    page_text = ocr_text\n",
    "                            \n",
    "                            if page_text.strip():\n",
    "                                texto_completo += page_text + \"\\n\\n\"\n",
    "                        \n",
    "                        if texto_completo.strip():\n",
    "                            documentos_carregados.append(\n",
    "                                Document(\n",
    "                                    page_content=texto_completo, \n",
    "                                    metadata={\"source\": caminho_arquivo_pdf, \"cached\": False}\n",
    "                                )\n",
    "                            )\n",
    "                            \n",
    "                            # Salvar no cache\n",
    "                            with open(cache_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                                f.write(texto_completo)\n",
    "                            print(f\"    ğŸ’¾ Salvo no cache\")\n",
    "                        \n",
    "                        doc.close()\n",
    "                    except Exception as e:\n",
    "                        print(f\"  âŒ Erro ao processar '{file_name}': {e}\")\n",
    "        \n",
    "        print(f\"âœ… Total: {len(documentos_carregados)} documentos carregados\")\n",
    "        return documentos_carregados\n",
    "    \n",
    "    # --- NÃ“NS DO GRAFO ---\n",
    "    def _load_documents_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"NÃ³ para carregar documentos\"\"\"\n",
    "        if not os.path.exists(self.docs_directory) or not os.listdir(self.docs_directory):\n",
    "            return {\n",
    "                \"retriever_initialized\": False, \n",
    "                \"answer\": f\"âŒ DiretÃ³rio '{self.docs_directory}' vazio ou inexistente.\"\n",
    "            }\n",
    "        \n",
    "        docs = self._load_documents_with_cache()\n",
    "        \n",
    "        if not docs:\n",
    "            return {\n",
    "                \"retriever_initialized\": False, \n",
    "                \"answer\": \"âŒ Nenhum documento vÃ¡lido encontrado.\"\n",
    "            }\n",
    "        \n",
    "        # Dividir documentos\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500, \n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        \n",
    "        # Criar retriever\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=split_docs, \n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "        \n",
    "        # Atualizar sessÃ£o\n",
    "        self.session.retriever = retriever\n",
    "        self.session.retriever_initialized = True\n",
    "        self.session.documents_loaded = True\n",
    "        \n",
    "        return {\n",
    "            \"documents\": split_docs,\n",
    "            \"retriever\": retriever,\n",
    "            \"retriever_initialized\": True,\n",
    "            \"answer\": \"\"\n",
    "        }\n",
    "    \n",
    "    def _retrieve_documents_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"NÃ³ para buscar documentos relevantes\"\"\"\n",
    "        question = state[\"question\"]\n",
    "        \n",
    "        # Usar retriever da sessÃ£o se disponÃ­vel\n",
    "        retriever = self.session.retriever if self.session.retriever_initialized else state.get(\"retriever\")\n",
    "        \n",
    "        if retriever is None:\n",
    "            return {\n",
    "                \"documents\": [], \n",
    "                \"answer\": \"âŒ Retriever nÃ£o configurado.\"\n",
    "            }\n",
    "        \n",
    "        documents_for_qa = retriever.invoke(question)\n",
    "        return {\"documents\": documents_for_qa}\n",
    "\n",
    "    def _filter_relevant_documents(self, documents: List[Document], question: str) -> List[Document]:\n",
    "        \"\"\"Filtra documentos por relevÃ¢ncia usando embeddings\"\"\"\n",
    "        question_embedding = self.embeddings.embed_query(question)\n",
    "        scored_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_embedding = self.embeddings.embed_query(doc.page_content)\n",
    "            similarity = np.dot(question_embedding, doc_embedding)\n",
    "            scored_docs.append((similarity, doc))\n",
    "        \n",
    "        # Ordena por similaridade e pega os mais relevantes\n",
    "        scored_docs.sort(reverse=True, key=lambda x: x[0])\n",
    "        return [doc for _, doc in scored_docs[:3]]  # Apenas os 3 mais relevantes\n",
    "    \n",
    "    def _generate_answer_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"NÃ³ para gerar resposta com todas as otimizaÃ§Ãµes\"\"\"\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "        \n",
    "        if not documents:\n",
    "            return {\"answer\": \"âŒ Nenhum documento relevante encontrado.\"}\n",
    "        \n",
    "        # Filtra documentos por relevÃ¢ncia\n",
    "        relevant_docs = self._filter_relevant_documents(documents, question)\n",
    "        \n",
    "        # Trunca documentos para caber no limite do modelo\n",
    "        truncated_docs = self._truncate_documents(relevant_docs, max_tokens=60000)\n",
    "        \n",
    "        # Prompt especializado para condomÃ­nio\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        VocÃª Ã© um assistente especializado em Q&A para o CondomÃ­nio Solar Trindade.\n",
    "        Use os documentos fornecidos (atas, contratos, comunicados, regulamentos) para responder Ã  pergunta.\n",
    "        \n",
    "        INSTRUÃ‡Ã•ES:\n",
    "        - Seja preciso e especÃ­fico nas datas e informaÃ§Ãµes\n",
    "        - Se nÃ£o souber, diga \"NÃ£o tenho informaÃ§Ãµes suficientes nos documentos\"\n",
    "        - Para listas, use formataÃ§Ã£o clara com numeraÃ§Ã£o\n",
    "        - Mantenha tom profissional mas acessÃ­vel\n",
    "        \n",
    "        Contexto dos documentos: {context}\n",
    "        \n",
    "        Pergunta: {input}\n",
    "        \n",
    "        Resposta:\n",
    "        \"\"\")\n",
    "        \n",
    "        document_chain = create_stuff_documents_chain(self.llm, prompt)\n",
    "        try:\n",
    "            response = document_chain.invoke({\n",
    "                \"input\": question, \n",
    "                \"context\": truncated_docs\n",
    "            })\n",
    "            return {\"answer\": response}\n",
    "        except Exception as e:\n",
    "            return {\"answer\": f\"âŒ Erro ao gerar resposta: {str(e)}\"}\n",
    "        \n",
    "    \n",
    "    def _decide_next_step(self, state: AgentState) -> str:\n",
    "        \"\"\"Decide prÃ³ximo passo baseado no estado\"\"\"\n",
    "        # Verificar se retriever estÃ¡ inicializado na sessÃ£o ou no estado\n",
    "        if self.session.retriever_initialized or state.get(\"retriever_initialized\"):\n",
    "            return \"retrieve_docs\"\n",
    "        else:\n",
    "            return \"load_docs\"\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        \"\"\"ConstrÃ³i o grafo LangGraph\"\"\"\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Adicionar nÃ³s\n",
    "        workflow.add_node(\"decide_initial_step\", lambda state: state)\n",
    "        workflow.add_node(\"load_docs\", self._load_documents_node)\n",
    "        workflow.add_node(\"retrieve_docs\", self._retrieve_documents_node)\n",
    "        workflow.add_node(\"generate_answer\", self._generate_answer_node)\n",
    "        \n",
    "        # Configurar fluxo\n",
    "        workflow.set_entry_point(\"decide_initial_step\")\n",
    "        \n",
    "        workflow.add_conditional_edges(\n",
    "            \"decide_initial_step\",\n",
    "            self._decide_next_step,\n",
    "            {\n",
    "                \"load_docs\": \"load_docs\",\n",
    "                \"retrieve_docs\": \"retrieve_docs\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        workflow.add_edge(\"load_docs\", \"retrieve_docs\")\n",
    "        workflow.add_edge(\"retrieve_docs\", \"generate_answer\")\n",
    "        workflow.add_edge(\"generate_answer\", END)\n",
    "        \n",
    "        self.app = workflow.compile()\n",
    "    \n",
    "    # --- INTERFACE PÃšBLICA ---\n",
    "    def ask_question(self, question: str, show_process: bool = False) -> str:\n",
    "        \"\"\"Faz uma pergunta ao agente\"\"\"\n",
    "        print(f\"â“ {question}\")\n",
    "        \n",
    "        if show_process:\n",
    "            print(\"ğŸ”„ Processando...\")\n",
    "        \n",
    "        # Preparar estado inicial\n",
    "        initial_state = {\n",
    "            \"question\": question,\n",
    "            \"documents\": [],\n",
    "            \"answer\": \"\",\n",
    "            \"retriever\": self.session.retriever,\n",
    "            \"retriever_initialized\": self.session.retriever_initialized,\n",
    "            \"conversation_history\": self.session.conversation_history,\n",
    "            \"session_context\": self.session\n",
    "        }\n",
    "        \n",
    "        # Executar grafo\n",
    "        current_result = None\n",
    "        for step in self.app.stream(initial_state):\n",
    "            if show_process:\n",
    "                node_name = list(step.keys())[0] if step else \"unknown\"\n",
    "                print(f\"  ğŸ”¸ {node_name}\")\n",
    "            current_result = step\n",
    "        \n",
    "        # Processar resultado\n",
    "        if current_result:\n",
    "            final_state = list(current_result.values())[0]\n",
    "            answer = final_state.get(\"answer\", \"âŒ Erro ao processar pergunta\")\n",
    "            \n",
    "            # Atualizar histÃ³rico\n",
    "            qa_pair = {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            self.session.conversation_history.append(qa_pair)\n",
    "            \n",
    "            return answer\n",
    "        \n",
    "        return \"âŒ Erro no processamento\"\n",
    "    \n",
    "    def ask_and_display(self, question: str, show_process: bool = False):\n",
    "        \"\"\"Faz pergunta e exibe resposta formatada\"\"\"\n",
    "        answer = self.ask_question(question, show_process)\n",
    "        print(f\"\\nğŸ’¡ **Resposta:**\")\n",
    "        display(Markdown(answer))\n",
    "        return answer\n",
    "    \n",
    "    def show_conversation_history(self, limit: int = 5):\n",
    "        \"\"\"Mostra histÃ³rico de conversas\"\"\"\n",
    "        print(f\"\\nğŸ“‹ **HistÃ³rico de Conversas** (Ãºltimas {limit}):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        recent = self.session.conversation_history[-limit:]\n",
    "        \n",
    "        for i, qa in enumerate(recent, 1):\n",
    "            timestamp = qa.get(\"timestamp\", \"N/A\")\n",
    "            question = qa.get(\"question\", \"\")\n",
    "            answer = qa.get(\"answer\", \"\")\n",
    "            \n",
    "            print(f\"\\n**{i}.** ğŸ•’ {timestamp}\")\n",
    "            print(f\"â“ {question}\")\n",
    "            print(f\"ğŸ’¡ {answer[:100]}{'...' if len(answer) > 100 else ''}\")\n",
    "    \n",
    "    def get_session_info(self):\n",
    "        \"\"\"InformaÃ§Ãµes da sessÃ£o\"\"\"\n",
    "        return {\n",
    "            \"session_id\": self.session.session_id,\n",
    "            \"total_questions\": len(self.session.conversation_history),\n",
    "            \"retriever_initialized\": self.session.retriever_initialized,\n",
    "            \"documents_loaded\": self.session.documents_loaded,\n",
    "            \"docs_directory\": self.docs_directory,\n",
    "            \"cache_directory\": self.cache_dir\n",
    "        }\n",
    "\n",
    "# --- EXECUÃ‡ÃƒO PRINCIPAL ---\n",
    "def run_solar_qa_demo(provider: str = \"openai\"):\n",
    "    \"\"\"Executa demonstraÃ§Ã£o do agente Solar Q&A\"\"\"\n",
    "    \n",
    "    print(\"ğŸ¢ === AGENTE Q&A CONDOMÃNIO SOLAR TRINDADE ===\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Inicializar agente\n",
    "    qa_agent = SolarCondominiumQA(provider=provider)\n",
    "    \n",
    "    # Perguntas de exemplo\n",
    "    sample_questions = [\n",
    "        \"Qual a data da Ãºltima reuniÃ£o ou assembleia do condomÃ­nio?\",\n",
    "        #\"Quais as datas das cinco Ãºltimas reuniÃµes ou assembleias?\",\n",
    "        \"Quais foram os principais assuntos da Ãºltima assembleia?\",\n",
    "        \"quais os nomes dos membros do conselho fiscal atuais?\",\n",
    "        #\"Como base na ultima assembleia, qual o valor da taxa condominial atual?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ“Š **InformaÃ§Ãµes da SessÃ£o:**\")\n",
    "    for key, value in qa_agent.get_session_info().items():\n",
    "        print(f\"  â€¢ {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Executar perguntas\n",
    "    for i, question in enumerate(sample_questions, 1):\n",
    "        print(f\"\\nğŸ”¸ **Pergunta {i}/{len(sample_questions)}**\")\n",
    "        qa_agent.ask_and_display(question, show_process=(i == 1))\n",
    "        \n",
    "        if i < len(sample_questions):\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "    \n",
    "    # Mostrar histÃ³rico\n",
    "    qa_agent.show_conversation_history()\n",
    "    \n",
    "    print(f\"\\nâœ… **SessÃ£o Finalizada:** {qa_agent.session.session_id}\")\n",
    "    print(f\"ğŸ“ˆ **Total de Perguntas:** {len(qa_agent.session.conversation_history)}\")\n",
    "    \n",
    "    return qa_agent\n",
    "\n",
    "def interactive_solar_qa():\n",
    "    \"\"\"Modo interativo para o Solar Q&A\"\"\"\n",
    "    qa_agent = SolarCondominiumQA()\n",
    "    \n",
    "    print(\"ğŸ¢ === MODO INTERATIVO - SOLAR TRINDADE ===\")\n",
    "    print(\"Comandos: 'sair', 'historico', 'info', 'limpar'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\nâ“ Sua pergunta: \").strip()\n",
    "            \n",
    "            if question.lower() in ['sair', 'exit', 'quit']:\n",
    "                print(\"ğŸ‘‹ Encerrando...\")\n",
    "                break\n",
    "            elif question.lower() in ['historico', 'history']:\n",
    "                qa_agent.show_conversation_history()\n",
    "                continue\n",
    "            elif question.lower() == 'info':\n",
    "                print(\"\\nğŸ“Š **InformaÃ§Ãµes da SessÃ£o:**\")\n",
    "                for key, value in qa_agent.get_session_info().items():\n",
    "                    print(f\"  â€¢ {key}: {value}\")\n",
    "                continue\n",
    "            elif question.lower() in ['limpar', 'clear']:\n",
    "                qa_agent.session.conversation_history = []\n",
    "                print(\"ğŸ—‘ï¸ HistÃ³rico limpo\")\n",
    "                continue\n",
    "            elif not question:\n",
    "                continue\n",
    "            \n",
    "            qa_agent.ask_and_display(question)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nğŸ‘‹ Interrompido pelo usuÃ¡rio\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erro: {e}\")\n",
    "    \n",
    "    return qa_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aa9b280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”§ Inicializando LLM: openai\n",
      "ğŸ¢ === AGENTE Q&A CONDOMÃNIO SOLAR TRINDADE ===\n",
      "============================================================\n",
      "ğŸ”§ Inicializando LLM com : openai\n",
      "\n",
      "============================================================\n",
      "ğŸ”§ Inicializando LLM OpenAI (gpt-4o-mini)...\n",
      "ğŸ“Š **InformaÃ§Ãµes da SessÃ£o:**\n",
      "  â€¢ session_id: session_20250620_191404\n",
      "  â€¢ total_questions: 0\n",
      "  â€¢ retriever_initialized: False\n",
      "  â€¢ documents_loaded: False\n",
      "  â€¢ docs_directory: docs_condominio\n",
      "  â€¢ cache_directory: processed_docs_cache\n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ”¸ **Pergunta 1/3**\n",
      "â“ Qual a data da Ãºltima reuniÃ£o ou assembleia do condomÃ­nio?\n",
      "ğŸ”„ Processando...\n",
      "  ğŸ”¸ decide_initial_step\n",
      "ğŸ“š Carregando documentos de: docs_condominio\n",
      "âœ… Total: 31 documentos carregados\n",
      "  ğŸ”¸ load_docs\n",
      "  ğŸ”¸ retrieve_docs\n",
      "â„¹ï¸ Documentos truncados para 71 tokens (limite: 60000)\n",
      "  ğŸ”¸ generate_answer\n",
      "\n",
      "ğŸ’¡ **Resposta:**\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A Ãºltima reuniÃ£o do CondomÃ­nio Solar Trindade ocorreu no dia 6 de agosto de 2024, em segunda convocaÃ§Ã£o, Ã s 20h, no salÃ£o de festas do condomÃ­nio."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”¸ **Pergunta 2/3**\n",
      "â“ Quais foram os principais assuntos da Ãºltima assembleia?\n",
      "â„¹ï¸ Documentos truncados para 49 tokens (limite: 60000)\n",
      "\n",
      "ğŸ’¡ **Resposta:**\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Na Ãºltima assembleia, realizada em 04 de novembro de 2024, os principais assuntos abordados foram:\n",
       "\n",
       "1. **AtualizaÃ§Ã£o do Regimento Interno**: Foi discutido que nÃ£o foi possÃ­vel encaminhar a votaÃ§Ã£o para a aprovaÃ§Ã£o da atualizaÃ§Ã£o do Regimento Interno devido a pendÃªncias. Decidiu-se agendar uma nova assembleia para tratar desse assunto.\n",
       "\n",
       "2. **PrevisÃ£o OrÃ§amentÃ¡ria para 2024**: A Sra. Maria Fernanda, representante da Controlar, apresentou e explicou item a item da minuta da previsÃ£o orÃ§amentÃ¡ria para o ano de 2024, considerando o reajuste dos funcionÃ¡rios e a inflaÃ§Ã£o.\n",
       "\n",
       "3. **Itens Delicados**: A sÃ­ndica mencionou que os itens 4, 5 e 6 da pauta eram delicados e que seria necessÃ¡rio dedicar mais tempo a esses assuntos, alÃ©m do tempo mÃ©dio de dez minutos previsto para a assembleia.\n",
       "\n",
       "Esses foram os principais tÃ³picos discutidos na assembleia."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”¸ **Pergunta 3/3**\n",
      "â“ quais os nomes dos membros do conselho fiscal atuais?\n",
      "â„¹ï¸ Documentos truncados para 38 tokens (limite: 60000)\n",
      "\n",
      "ğŸ’¡ **Resposta:**\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "NÃ£o tenho informaÃ§Ãµes suficientes nos documentos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ **HistÃ³rico de Conversas** (Ãºltimas 5):\n",
      "------------------------------------------------------------\n",
      "\n",
      "**1.** ğŸ•’ 2025-06-20T19:22:31.679589\n",
      "â“ Qual a data da Ãºltima reuniÃ£o ou assembleia do condomÃ­nio?\n",
      "ğŸ’¡ A Ãºltima reuniÃ£o do CondomÃ­nio Solar Trindade ocorreu no dia 6 de agosto de 2024, em segunda convoca...\n",
      "\n",
      "**2.** ğŸ•’ 2025-06-20T19:22:36.961485\n",
      "â“ Quais foram os principais assuntos da Ãºltima assembleia?\n",
      "ğŸ’¡ Na Ãºltima assembleia, realizada em 04 de novembro de 2024, os principais assuntos abordados foram:\n",
      "\n",
      "...\n",
      "\n",
      "**3.** ğŸ•’ 2025-06-20T19:22:39.469336\n",
      "â“ quais os nomes dos membros do conselho fiscal atuais?\n",
      "ğŸ’¡ NÃ£o tenho informaÃ§Ãµes suficientes nos documentos.\n",
      "\n",
      "âœ… **SessÃ£o Finalizada:** session_20250620_191404\n",
      "ğŸ“ˆ **Total de Perguntas:** 3\n",
      "\n",
      "ğŸ”§ Inicializando LLM: gemini\n",
      "ğŸ¢ === AGENTE Q&A CONDOMÃNIO SOLAR TRINDADE ===\n",
      "============================================================\n",
      "ğŸ”§ Inicializando LLM com : gemini\n",
      "\n",
      "============================================================\n",
      "ğŸ”§ Inicializando LLM Google Gemini...\n",
      "ğŸ“Š **InformaÃ§Ãµes da SessÃ£o:**\n",
      "  â€¢ session_id: session_20250620_192239\n",
      "  â€¢ total_questions: 0\n",
      "  â€¢ retriever_initialized: False\n",
      "  â€¢ documents_loaded: False\n",
      "  â€¢ docs_directory: docs_condominio\n",
      "  â€¢ cache_directory: processed_docs_cache\n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ”¸ **Pergunta 1/3**\n",
      "â“ Qual a data da Ãºltima reuniÃ£o ou assembleia do condomÃ­nio?\n",
      "ğŸ”„ Processando...\n",
      "  ğŸ”¸ decide_initial_step\n",
      "ğŸ“š Carregando documentos de: docs_condominio\n",
      "âœ… Total: 31 documentos carregados\n",
      "  ğŸ”¸ load_docs\n",
      "  ğŸ”¸ retrieve_docs\n",
      "â„¹ï¸ Documentos truncados para 72 tokens (limite: 60000)\n",
      "  ğŸ”¸ generate_answer\n",
      "\n",
      "ğŸ’¡ **Resposta:**\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A Ãºltima Assembleia Geral ExtraordinÃ¡ria do CondomÃ­nio do EdifÃ­cio Solar Trindade, registrada nos documentos fornecidos, foi realizada no dia 25 de janeiro de 2023."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”¸ **Pergunta 2/3**\n",
      "â“ Quais foram os principais assuntos da Ãºltima assembleia?\n",
      "â„¹ï¸ Documentos truncados para 42 tokens (limite: 60000)\n",
      "\n",
      "ğŸ’¡ **Resposta:**\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Na assembleia de 04 de novembro de 2024, o principal assunto foi a atualizaÃ§Ã£o do Regimento Interno do CondomÃ­nio Solar Trindade. Devido a pendÃªncias destacadas em sessÃµes anteriores, nÃ£o foi possÃ­vel encaminhar a votaÃ§Ã£o para aprovaÃ§Ã£o ou nÃ£o da atualizaÃ§Ã£o. Ficou decidido que uma nova assembleia serÃ¡ agendada para resolver as pendÃªncias e votar o documento final."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”¸ **Pergunta 3/3**\n",
      "â“ quais os nomes dos membros do conselho fiscal atuais?\n",
      "â„¹ï¸ Documentos truncados para 38 tokens (limite: 60000)\n",
      "\n",
      "ğŸ’¡ **Resposta:**\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "NÃ£o tenho informaÃ§Ãµes suficientes nos documentos para fornecer os nomes dos membros do conselho fiscal atuais. Os documentos informam apenas sobre o processo de eleiÃ§Ã£o e composiÃ§Ã£o do Conselho Fiscal."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ **HistÃ³rico de Conversas** (Ãºltimas 5):\n",
      "------------------------------------------------------------\n",
      "\n",
      "**1.** ğŸ•’ 2025-06-20T19:31:05.556944\n",
      "â“ Qual a data da Ãºltima reuniÃ£o ou assembleia do condomÃ­nio?\n",
      "ğŸ’¡ A Ãºltima Assembleia Geral ExtraordinÃ¡ria do CondomÃ­nio do EdifÃ­cio Solar Trindade, registrada nos do...\n",
      "\n",
      "**2.** ğŸ•’ 2025-06-20T19:31:07.880871\n",
      "â“ Quais foram os principais assuntos da Ãºltima assembleia?\n",
      "ğŸ’¡ Na assembleia de 04 de novembro de 2024, o principal assunto foi a atualizaÃ§Ã£o do Regimento Interno ...\n",
      "\n",
      "**3.** ğŸ•’ 2025-06-20T19:31:10.161486\n",
      "â“ quais os nomes dos membros do conselho fiscal atuais?\n",
      "ğŸ’¡ NÃ£o tenho informaÃ§Ãµes suficientes nos documentos para fornecer os nomes dos membros do conselho fisc...\n",
      "\n",
      "âœ… **SessÃ£o Finalizada:** session_20250620_192239\n",
      "ğŸ“ˆ **Total de Perguntas:** 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- EXECUÃ‡ÃƒO ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Escolha o modo de execuÃ§Ã£o:\n",
    "    # qa_agent = run_solar_qa_demo(\"claude\")     # Demo automÃ¡tica\n",
    "    # qa_agent = interactive_solar_qa()   # Modo interativo\n",
    "\n",
    "    providers = [\n",
    "        \"openai\", \n",
    "        \"gemini\", \n",
    "        # \"claude\",\n",
    "        # \"groq_llama3\", \n",
    "        # \"groq_gemma\", \n",
    "        # \"groq_mistral\", \n",
    "        # \"deepseek\"\n",
    "    ]\n",
    "\n",
    "    for provider in providers:\n",
    "        print(f\"\\nğŸ”§ Inicializando LLM: {provider}\")\n",
    "        run_solar_qa_demo(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "429c1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_agent = SolarCondominiumQA()\n",
    "# # resposta = qa_agent.ask_question(\"Qual a Ãºltima assembleia? Resposta curta e resumida\")\n",
    "# resposta = qa_agent.ask_question(\"Quem sÃ£o os membros do conselho fiscal? Resposta curta e resumida\")\n",
    "# qa_agent.show_conversation_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
